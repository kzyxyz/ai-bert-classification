# export_and_test_coreml_detailed.py
"""
åŠŸèƒ½è¯´æ˜ï¼š
å°†å¾®è°ƒå®Œæˆçš„RBT3æ¨¡å‹è½¬æ¢ä¸ºCoreMLæ ¼å¼
ä¿®å¤NaNè¾“å‡ºé—®é¢˜
"""

import os
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, AutoConfig, PreTrainedModel, AutoModelForSequenceClassification
from transformers import BertConfig
from safetensors.torch import load_file
import coremltools as ct
import numpy as np

# ================== 1ï¸âƒ£ é…ç½®å‚æ•° ==================
# å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„
pytorch_model_path = "./chinese_roberta_L-4_H-256-detector-final"

# Core ML è¾“å‡ºç›®å½•
output_dir = "out_coreml_sys_fp32"
os.makedirs(output_dir, exist_ok=True)

coreml_model_path = os.path.join(output_dir, "TextClassifier.mlpackage")
max_length = 128
use_fp16 = True  # å…ˆä½¿ç”¨FP32é¿å…NaNé—®é¢˜

# ================== 2ï¸âƒ£ åŠ è½½å¾®è°ƒåçš„æ¨¡å‹ ==================
print("ğŸ”„ æ­£åœ¨åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...")

try:
    # ç›´æ¥åŠ è½½å¾®è°ƒåçš„å®Œæ•´æ¨¡å‹
    model = AutoModelForSequenceClassification.from_pretrained(pytorch_model_path)
    tokenizer = AutoTokenizer.from_pretrained(pytorch_model_path)
    print("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸ")

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(f"   æ¨¡å‹ç±»å‹: {model.config.model_type}")
    print(f"   åˆ†ç±»æ•°é‡: {model.config.num_labels}")
    print(f"   æ ‡ç­¾æ˜ å°„: {model.config.id2label}")
    print(f"   éšè—å±‚å¤§å°: {model.config.hidden_size}")
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

except Exception as e:
    print(f"âŒ å¾®è°ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit(1)

model.eval()
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# ================== 5ï¸âƒ£ å…ˆæµ‹è¯•PyTorchæ¨¡å‹æ˜¯å¦æ­£å¸¸ ==================
print("\nğŸ§ª å…ˆæµ‹è¯•PyTorchæ¨¡å‹...")


def test_pytorch_model(text):
    inputs = tokenizer(
        text,
        max_length=max_length,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )

    with torch.no_grad():
        outputs = model(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"])
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=-1)
        pred_label = torch.argmax(probs, dim=1).item()
        confidence = probs[0][pred_label].item()

    return pred_label, confidence, probs[0].numpy()


test_texts = ["å…¬å¸è´¢åŠ¡æŠ¥å‘Šéœ€è¦ä¿å¯†", "ä»Šå¤©å¤©æ°”å¾ˆå¥½é€‚åˆå¤–å‡º"]
for text in test_texts:
    label, confidence, probs = test_pytorch_model(text)
    prediction = model.config.id2label[label]  # ä½¿ç”¨æ¨¡å‹çš„æ ‡ç­¾æ˜ å°„
    print(f"PyTorch - æ–‡æœ¬: {text}")
    print(f"PyTorch - é¢„æµ‹: {prediction} (ç½®ä¿¡åº¦: {confidence:.4f})")
    print(f"PyTorch - æ¦‚ç‡åˆ†å¸ƒ: éæ•æ„Ÿ({probs[0]:.4f}), æ•æ„Ÿ({probs[1]:.4f})")
    print()

# ================== 6ï¸âƒ£ TorchScriptè¿½è¸ª ==================
print("ğŸ”„ æ­£åœ¨è½¬æ¢ä¸ºTorchScript...")


class TraceWrapper(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids, attention_mask):
        # ç¡®ä¿åªè¿”å›logits
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.logits


wrapped_model = TraceWrapper(model)
wrapped_model.eval()  # ç¡®ä¿åœ¨evalæ¨¡å¼

# å‡†å¤‡ç¤ºä¾‹è¾“å…¥
sample_text = "æµ‹è¯•æ–‡æœ¬"
inputs = tokenizer(
    sample_text,
    max_length=max_length,
    padding="max_length",
    truncation=True,
    return_tensors="pt"
)

print(f"è¾“å…¥å½¢çŠ¶: input_ids {inputs['input_ids'].shape}, attention_mask {inputs['attention_mask'].shape}")

with torch.no_grad():
    traced_model = torch.jit.trace(wrapped_model, (inputs["input_ids"], inputs["attention_mask"]))

# ================== 7ï¸âƒ£ CoreMLè½¬æ¢ ==================
print("ğŸ”„ æ­£åœ¨è½¬æ¢ä¸ºCoreML...")

# ä½¿ç”¨FP32é¿å…NaNé—®é¢˜
input_ids_desc = ct.TensorType(
    name="input_ids",
    shape=ct.Shape(shape=(
        1, ct.RangeDim(lower_bound=1, upper_bound=max_length),
    )),
    dtype=np.int32
)
  
attention_mask_desc = ct.TensorType(
    name="attention_mask",
    shape=ct.Shape(shape=(
        1, ct.RangeDim(lower_bound=1, upper_bound=max_length),
    )),
    dtype=np.int32
)

# å…ˆå°è¯•FP32è½¬æ¢
mlmodel = ct.convert(
    traced_model,
    inputs=[input_ids_desc, attention_mask_desc],
    outputs=[ct.TensorType(name="logits")],
    convert_to="mlprogram",
    compute_precision=ct.precision.FLOAT32,  # ä½¿ç”¨FP32
    compute_units=ct.ComputeUnit.CPU_ONLY,  # å…ˆç”¨CPUç¡®ä¿ç¨³å®šæ€§
    skip_model_load=False
)

# ================== 8ï¸âƒ£ ä¿å­˜æ¨¡å‹ ==================
mlmodel.save(coreml_model_path)
print(f"âœ… CoreMLæ¨¡å‹å·²ä¿å­˜: {coreml_model_path}")


# ================== 9ï¸âƒ£ æµ‹è¯•è½¬æ¢åçš„æ¨¡å‹ ==================
def encode_text(text):
    inputs = tokenizer(
        text,
        max_length=max_length,
        padding="max_length",
        truncation=True,
        return_tensors="np"
    )
    return {
        "input_ids": inputs["input_ids"].astype(np.int32),
        "attention_mask": inputs["attention_mask"].astype(np.int32)
    }


def predict_coreml(text):
    encoded = encode_text(text)
    output = mlmodel.predict(encoded)
    logits = output["logits"]

    # æ£€æŸ¥æ˜¯å¦æœ‰NaN
    if np.isnan(logits).any():
        print(f"âš ï¸ æ£€æµ‹åˆ°NaNå€¼: {logits}")
        return 0, 0.0, np.array([0.5, 0.5])

    pred_label = int(np.argmax(logits, axis=1)[0])

    # è®¡ç®—æ¦‚ç‡
    probs = torch.nn.functional.softmax(torch.from_numpy(logits), dim=-1)
    confidence = float(probs[0][pred_label])

    return pred_label, confidence, probs[0].numpy()


print("\nğŸ§ª æµ‹è¯•CoreMLæ¨¡å‹:")
test_texts = [
    "å…¬å¸è´¢åŠ¡æŠ¥å‘Šéœ€è¦ä¿å¯†",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½é€‚åˆå¤–å‡º",
    "å‘˜å·¥ä¸ªäººä¿¡æ¯è¡¨",
    "å…¬å¼€å¸‚åœºåˆ†ææŠ¥å‘Š",
    "æŠ¥ä»·8285ä¸‡ï¼Œé¡¹ç›®å†…éƒ¨"
]

for text in test_texts:
    label, confidence, probs = predict_coreml(text)
    prediction = model.config.id2label[label]  # ä½¿ç”¨æ¨¡å‹çš„æ ‡ç­¾æ˜ å°„
    print(f"CoreML - æ–‡æœ¬: {text}")
    print(f"CoreML - é¢„æµ‹: {prediction} (ç½®ä¿¡åº¦: {confidence:.4f})")
    if not np.isnan(probs).any():
        print(f"CoreML - æ¦‚ç‡åˆ†å¸ƒ: éæ•æ„Ÿ({probs[0]:.4f}), æ•æ„Ÿ({probs[1]:.4f})")
    print()

print("ğŸ‰ CoreMLè½¬æ¢å®Œæˆ!")
