# export_and_test_coreml_detailed.py
"""
åŠŸèƒ½è¯´æ˜ï¼š
1ï¸âƒ£ å°†å¾®è°ƒå®Œæˆçš„ TinyBERT è½¬æ¢ä¸º Core ML (.mlmodel æˆ– .mlpackage)
2ï¸âƒ£ è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•
3ï¸âƒ£ FP16 å‹ç¼©ï¼ˆå‡å°æ¨¡å‹ä½“ç§¯ï¼Œç²¾åº¦å‡ ä¹ä¸æŸå¤±ï¼‰
4ï¸âƒ£ ä¿®å¤ TorchScript dict è¾“å‡ºæŠ¥é”™é—®é¢˜ï¼ˆç›´æ¥è¿”å› logitsï¼‰
5ï¸âƒ£ æä¾›ç¤ºä¾‹æ–‡æœ¬æµ‹è¯•æ¨¡å‹é¢„æµ‹ç»“æœï¼ˆ0/1 äºŒåˆ†ç±»ï¼‰
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import coremltools as ct
import numpy as np

# ================== 1ï¸âƒ£ é…ç½®å‚æ•° ==================
# å¾®è°ƒåçš„ PyTorch æ¨¡å‹è·¯å¾„
pytorch_model_path = "./chinese_roberta_L-4_H-256-detector-final"

# Core ML è¾“å‡ºç›®å½•
output_dir = "out_coreml_sys_fp16"
os.makedirs(output_dir, exist_ok=True)  # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»º

# Core ML æ–‡ä»¶å
coreml_model_filename = "TextClassifier.mlpackage"
coreml_model_path = os.path.join(output_dir, coreml_model_filename)

# æ–‡æœ¬æœ€å¤§é•¿åº¦ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
max_length = 128

# æ˜¯å¦ä½¿ç”¨ FP16 å‹ç¼©ï¼ˆTrue: æ¨¡å‹ä½“ç§¯å‡åŠï¼Œç²¾åº¦å½±å“å¾ˆå°ï¼‰
use_fp16 = True

# ================== 2ï¸âƒ£ åŠ è½½å¾®è°ƒåçš„ PyTorch æ¨¡å‹ ==================
# AutoModelForSequenceClassification å¸¦æœ‰åˆ†ç±»å¤´ï¼ˆé€‚åˆäºŒåˆ†ç±»ä»»åŠ¡ï¼‰
model = AutoModelForSequenceClassification.from_pretrained(pytorch_model_path)
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œé¿å… dropout ç­‰è®­ç»ƒè¡Œä¸º

# ================== 3ï¸âƒ£ åˆ›å»ºç¤ºä¾‹è¾“å…¥ ==================
# åŠ è½½ tokenizerï¼ˆåˆ†è¯å™¨ï¼‰
tokenizer = AutoTokenizer.from_pretrained(pytorch_model_path)

# ç¤ºä¾‹æ–‡æœ¬ï¼Œç”¨äºç”Ÿæˆ TorchScript æ—¶çš„ trace
sample_text = "è¿™æ˜¯ä¸€ä¸ªæ•æ„Ÿæ–‡æœ¬æµ‹è¯•"

# tokenizer å°†æ–‡æœ¬è½¬ä¸ºæ¨¡å‹å¯è¯†åˆ«çš„ input_ids å’Œ attention_mask
inputs = tokenizer(
    sample_text,
    max_length=max_length,  # æœ€å¤§é•¿åº¦
    padding="max_length",   # ä¸è¶³ max_length ç”¨ 0 å¡«å……
    truncation=True,        # è¶…è¿‡ max_length æˆªæ–­
    return_tensors="pt"     # è¿”å› PyTorch tensor
)

# TorchScript è¿½è¸ªæ—¶éœ€è¦ tuple è¾“å…¥
example_inputs = (inputs["input_ids"], inputs["attention_mask"])

# ================== 4ï¸âƒ£ åŒ…è£…æ¨¡å‹ï¼Œè§£å†³ dict è¾“å‡ºé—®é¢˜ ==================
class TraceWrapper(torch.nn.Module):
    """
    ç”¨äº TorchScript è¿½è¸ªåŒ…è£…æ¨¡å‹
    åŸå§‹æ¨¡å‹ forward è¿”å› dictï¼ŒTorchScript å¯¹ dict è¾“å‡ºè¿½è¸ªå®¹æ˜“æŠ¥é”™
    è¿™é‡Œåªè¿”å› logitsï¼ˆæ¨¡å‹è¾“å‡ºå¼ é‡ï¼‰
    """
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids, attention_mask):
        # è¿”å› logits å¼ é‡ï¼Œshape = [batch_size, num_labels]
        return self.model(input_ids=input_ids, attention_mask=attention_mask).logits

wrapped_model = TraceWrapper(model)

# ================== 5ï¸âƒ£ TorchScript è¿½è¸ª ==================
# TorchScript å¯ä»¥å°† PyTorch æ¨¡å‹è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å½¢å¼ï¼Œä¾¿äº Core ML è½¬æ¢
with torch.no_grad():  # ä¸éœ€è¦è®¡ç®—æ¢¯åº¦
    traced_model = torch.jit.trace(wrapped_model, example_inputs)

# ================== 6ï¸âƒ£ Core ML è½¬æ¢ ==================
# convert_to="mlprogram" â†’ ä½¿ç”¨æœ€æ–° Core ML è¿è¡Œæ—¶
# compute_precision â†’ FP16 æˆ– FP32ï¼ŒFP16 å¯ä»¥å‡å°æ¨¡å‹ä½“ç§¯
mlmodel = ct.convert(
    traced_model,
    inputs=[
        ct.TensorType(
            name="input_ids",
            shape=inputs["input_ids"].shape,  # è¾“å…¥ shape
            dtype=np.int32                     # PyTorch token è½¬ä¸º Core ML int32
        ),
        ct.TensorType(
            name="attention_mask",
            shape=inputs["attention_mask"].shape,
            dtype=np.int32
        )
    ],
    outputs=[
        ct.TensorType(name="logits")  # è¾“å‡º logits åç§°ï¼Œé¿å… KeyError
    ],
    convert_to="mlprogram",
    compute_precision=ct.precision.FLOAT16 if use_fp16 else ct.precision.FLOAT32
)

# ================== 7ï¸âƒ£ ä¿å­˜ Core ML æ¨¡å‹ ==================
mlmodel.save(coreml_model_path)
print(f"âœ… Core ML æ¨¡å‹å·²ç”Ÿæˆå¹¶ä¿å­˜åˆ° {coreml_model_path}")

# ================== 8ï¸âƒ£ å®šä¹‰æ–‡æœ¬ç¼–ç å‡½æ•° ==================
def encode_text(text):
    """
    å°†æ–‡æœ¬ç¼–ç ä¸º Core ML å¯æ¥å—çš„è¾“å…¥
    è¿”å› dictï¼Œkey å¯¹åº” Core ML è¾“å…¥åå­—
    """
    inputs = tokenizer(
        text,
        max_length=max_length,
        padding="max_length",
        truncation=True,
        return_tensors="np"  # è¿”å› numpy æ•°ç»„
    )
    # Core ML è¾“å…¥å¿…é¡»æ˜¯ int32 ç±»å‹
    return {
        "input_ids": inputs["input_ids"].astype(np.int32),
        "attention_mask": inputs["attention_mask"].astype(np.int32)
    }

# ================== 9ï¸âƒ£ å®šä¹‰é¢„æµ‹å‡½æ•° ==================
def predict(text):
    """
    è¾“å…¥æ–‡æœ¬ï¼Œè¿”å›é¢„æµ‹ç±»åˆ«ï¼ˆ0 æˆ– 1ï¼‰
    """
    encoded = encode_text(text)
    output = mlmodel.predict(encoded)
    # è¾“å‡º logits å¼ é‡
    logits = output["logits"]
    # argmax å¾—åˆ°é¢„æµ‹ç±»åˆ«
    pred_label = int(np.argmax(logits, axis=1)[0])
    return pred_label

# ================== ğŸ”Ÿ æµ‹è¯•æ¨¡å‹é¢„æµ‹ ==================
# test_texts = [
#     "è¯·ä¿å¯†ï¼šè¿™æ¡æ¶ˆæ¯åŒ…å«å†…éƒ¨ä¿¡æ¯",  # é¢„æœŸæ•æ„Ÿ â†’ 1
#     "æ™®é€šæ–‡æœ¬ï¼Œæ²¡æœ‰æ•æ„Ÿå†…å®¹"           # é¢„æœŸå®‰å…¨ â†’ 0
# ]
#
# print("\nâœ… æµ‹è¯• Core ML æ¨¡å‹é¢„æµ‹ç»“æœ:")
# for text in test_texts:
#     label = predict(text)
#     print(f"æ–‡æœ¬: {text}")
#     print(f"é¢„æµ‹ç±»åˆ«: {label}\n")  # è¾“å‡º 0 æˆ– 1

# ================== 9ï¸âƒ£ æµ‹è¯•è½¬æ¢åçš„æ¨¡å‹ ==================
def encode_text(text):
    inputs = tokenizer(
        text,
        max_length=max_length,
        padding="max_length",
        truncation=True,
        return_tensors="np"
    )
    return {
        "input_ids": inputs["input_ids"].astype(np.int32),
        "attention_mask": inputs["attention_mask"].astype(np.int32)
    }


def predict_coreml(text):
    encoded = encode_text(text)
    output = mlmodel.predict(encoded)
    logits = output["logits"]

    # æ£€æŸ¥æ˜¯å¦æœ‰NaN
    if np.isnan(logits).any():
        print(f"âš ï¸ æ£€æµ‹åˆ°NaNå€¼: {logits}")
        return 0, 0.0, np.array([0.5, 0.5])

    pred_label = int(np.argmax(logits, axis=1)[0])

    # è®¡ç®—æ¦‚ç‡
    probs = torch.nn.functional.softmax(torch.from_numpy(logits), dim=-1)
    confidence = float(probs[0][pred_label])

    return pred_label, confidence, probs[0].numpy()


print("\nğŸ§ª æµ‹è¯•CoreMLæ¨¡å‹:")
test_texts = [
    "å…¬å¸è´¢åŠ¡æŠ¥å‘Šéœ€è¦ä¿å¯†",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½é€‚åˆå¤–å‡º",
    "å‘˜å·¥ä¸ªäººä¿¡æ¯è¡¨",
    "å…¬å¼€å¸‚åœºåˆ†ææŠ¥å‘Š",
    "æŠ¥ä»·8285ä¸‡ï¼Œé¡¹ç›®å†…éƒ¨",
    "12321313213213",
    "msvvzvkvklvnkznvknvvnzvlpqdlpwqdlpdlpfkdasfodsfnxawgn111111",
    "erfiasfkafnkafafhewnfkanfhewqnfekwfqkewqnkqgnggkkdafekanfkaff",
    "qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq"
]

for text in test_texts:
    label, confidence, probs = predict_coreml(text)
    prediction = model.config.id2label[label]  # ä½¿ç”¨æ¨¡å‹çš„æ ‡ç­¾æ˜ å°„
    print(f"CoreML - æ–‡æœ¬: {text}")
    print(f"CoreML - é¢„æµ‹: {prediction} (ç½®ä¿¡åº¦: {confidence:.4f})")
    if not np.isnan(probs).any():
        print(f"CoreML - æ¦‚ç‡åˆ†å¸ƒ: éæ•æ„Ÿ({probs[0]:.4f}), æ•æ„Ÿ({probs[1]:.4f})")
    print()

print("ğŸ‰ CoreMLè½¬æ¢å®Œæˆ!")
